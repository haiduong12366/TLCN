{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69b346d6-1a7a-4f80-a2e3-a0a6abc7088b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting delta-spark==3.2.0\n",
      "  Downloading delta_spark-3.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: pyspark<3.6.0,>=3.5.0 in /usr/local/spark/python (from delta-spark==3.2.0) (3.5.0)\n",
      "Requirement already satisfied: importlib-metadata>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from delta-spark==3.2.0) (6.8.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata>=1.0.0->delta-spark==3.2.0) (3.17.0)\n",
      "Collecting py4j==0.10.9.7 (from pyspark<3.6.0,>=3.5.0->delta-spark==3.2.0)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading delta_spark-3.2.0-py3-none-any.whl (21 kB)\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: py4j, delta-spark\n",
      "Successfully installed delta-spark-3.2.0 py4j-0.10.9.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install delta-spark==3.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de3d556-b47a-44f5-a1fa-1d6d631cf4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data input\n",
      "+------------+--------------------+---------+------+--------------------+-----------------+\n",
      "|kafka_offset|            kafka_ts|kafka_key|before|               after|            topic|\n",
      "+------------+--------------------+---------+------+--------------------+-----------------+\n",
      "|           0|2024-11-05 04:18:...| {\"id\":1}|  NULL|{\"id\":1,\"name\":\"T...|tlcn.tlcn.teacher|\n",
      "|           0|2024-11-05 04:18:...| {\"id\":1}|  NULL|{\"id\":1,\"name\":\"D...|tlcn.tlcn.student|\n",
      "|           0|2024-11-05 04:18:...| {\"id\":1}|  NULL|{\"id\":1,\"name\":\"m...|tlcn.tlcn.subject|\n",
      "|           1|2024-11-05 04:18:...| {\"id\":2}|  NULL|{\"id\":2,\"name\":\"H...|tlcn.tlcn.subject|\n",
      "|           2|2024-11-05 04:18:...| {\"id\":3}|  NULL|{\"id\":3,\"name\":\"H...|tlcn.tlcn.subject|\n",
      "|           3|2024-11-05 04:18:...| {\"id\":4}|  NULL|{\"id\":4,\"name\":\"L...|tlcn.tlcn.subject|\n",
      "|           4|2024-11-05 04:18:...| {\"id\":5}|  NULL|{\"id\":5,\"name\":\"v...|tlcn.tlcn.subject|\n",
      "|           5|2024-11-05 04:18:...| {\"id\":6}|  NULL|{\"id\":6,\"name\":\"s...|tlcn.tlcn.subject|\n",
      "+------------+--------------------+---------+------+--------------------+-----------------+\n",
      "\n",
      "s3a://mysql/brozes/subject\n",
      "Save successfully\n",
      "s3a://mysql/brozes/teacher\n",
      "Save successfully\n",
      "s3a://mysql/brozes/student\n",
      "Save successfully\n",
      "Data input\n",
      "+------------+--------+---------+------+-----+-----+\n",
      "|kafka_offset|kafka_ts|kafka_key|before|after|topic|\n",
      "+------------+--------+---------+------+-----+-----+\n",
      "+------------+--------+---------+------+-----+-----+\n",
      "\n",
      "Data input\n",
      "+------------+--------------------+---------+------+--------------------+---------------+\n",
      "|kafka_offset|            kafka_ts|kafka_key|before|               after|          topic|\n",
      "+------------+--------------------+---------+------+--------------------+---------------+\n",
      "|           0|2024-11-05 06:29:...| {\"Id\":1}|  NULL|{\"Id\":1,\"MSSubCla...|tlcn.tlcn.train|\n",
      "|           1|2024-11-05 06:29:...| {\"Id\":2}|  NULL|{\"Id\":2,\"MSSubCla...|tlcn.tlcn.train|\n",
      "|           2|2024-11-05 06:29:...| {\"Id\":3}|  NULL|{\"Id\":3,\"MSSubCla...|tlcn.tlcn.train|\n",
      "|           3|2024-11-05 06:29:...| {\"Id\":4}|  NULL|{\"Id\":4,\"MSSubCla...|tlcn.tlcn.train|\n",
      "|           4|2024-11-05 06:29:...| {\"Id\":5}|  NULL|{\"Id\":5,\"MSSubCla...|tlcn.tlcn.train|\n",
      "|           5|2024-11-05 06:29:...| {\"Id\":6}|  NULL|{\"Id\":6,\"MSSubCla...|tlcn.tlcn.train|\n",
      "|           6|2024-11-05 06:29:...| {\"Id\":7}|  NULL|{\"Id\":7,\"MSSubCla...|tlcn.tlcn.train|\n",
      "|           7|2024-11-05 06:29:...| {\"Id\":8}|  NULL|{\"Id\":8,\"MSSubCla...|tlcn.tlcn.train|\n",
      "|           8|2024-11-05 06:29:...| {\"Id\":9}|  NULL|{\"Id\":9,\"MSSubCla...|tlcn.tlcn.train|\n",
      "+------------+--------------------+---------+------+--------------------+---------------+\n",
      "\n",
      "s3a://mysql/brozes/train\n",
      "Save successfully\n",
      "Data input\n",
      "+------------+--------------------+---------+------+--------------------+---------------+\n",
      "|kafka_offset|            kafka_ts|kafka_key|before|               after|          topic|\n",
      "+------------+--------------------+---------+------+--------------------+---------------+\n",
      "|           9|2024-11-05 06:29:...|{\"Id\":10}|  NULL|{\"Id\":10,\"MSSubCl...|tlcn.tlcn.train|\n",
      "|          10|2024-11-05 06:29:...|{\"Id\":11}|  NULL|{\"Id\":11,\"MSSubCl...|tlcn.tlcn.train|\n",
      "|          11|2024-11-05 06:29:...|{\"Id\":12}|  NULL|{\"Id\":12,\"MSSubCl...|tlcn.tlcn.train|\n",
      "|          12|2024-11-05 06:29:...|{\"Id\":13}|  NULL|{\"Id\":13,\"MSSubCl...|tlcn.tlcn.train|\n",
      "|          13|2024-11-05 06:29:...|{\"Id\":14}|  NULL|{\"Id\":14,\"MSSubCl...|tlcn.tlcn.train|\n",
      "|          14|2024-11-05 06:29:...|{\"Id\":15}|  NULL|{\"Id\":15,\"MSSubCl...|tlcn.tlcn.train|\n",
      "|          15|2024-11-05 06:29:...|{\"Id\":16}|  NULL|{\"Id\":16,\"MSSubCl...|tlcn.tlcn.train|\n",
      "|          16|2024-11-05 06:29:...|{\"Id\":17}|  NULL|{\"Id\":17,\"MSSubCl...|tlcn.tlcn.train|\n",
      "|          17|2024-11-05 06:29:...|{\"Id\":18}|  NULL|{\"Id\":18,\"MSSubCl...|tlcn.tlcn.train|\n",
      "|          18|2024-11-05 06:29:...|{\"Id\":19}|  NULL|{\"Id\":19,\"MSSubCl...|tlcn.tlcn.train|\n",
      "|          19|2024-11-05 06:29:...|{\"Id\":20}|  NULL|{\"Id\":20,\"MSSubCl...|tlcn.tlcn.train|\n",
      "|          20|2024-11-05 06:29:...|{\"Id\":21}|  NULL|{\"Id\":21,\"MSSubCl...|tlcn.tlcn.train|\n",
      "|          21|2024-11-05 06:29:...|{\"Id\":22}|  NULL|{\"Id\":22,\"MSSubCl...|tlcn.tlcn.train|\n",
      "|          22|2024-11-05 06:29:...|{\"Id\":23}|  NULL|{\"Id\":23,\"MSSubCl...|tlcn.tlcn.train|\n",
      "|          23|2024-11-05 06:29:...|{\"Id\":24}|  NULL|{\"Id\":24,\"MSSubCl...|tlcn.tlcn.train|\n",
      "|          24|2024-11-05 06:29:...|{\"Id\":25}|  NULL|{\"Id\":25,\"MSSubCl...|tlcn.tlcn.train|\n",
      "|          25|2024-11-05 06:29:...|{\"Id\":26}|  NULL|{\"Id\":26,\"MSSubCl...|tlcn.tlcn.train|\n",
      "|          26|2024-11-05 06:29:...|{\"Id\":27}|  NULL|{\"Id\":27,\"MSSubCl...|tlcn.tlcn.train|\n",
      "|          27|2024-11-05 06:29:...|{\"Id\":28}|  NULL|{\"Id\":28,\"MSSubCl...|tlcn.tlcn.train|\n",
      "|          28|2024-11-05 06:29:...|{\"Id\":29}|  NULL|{\"Id\":29,\"MSSubCl...|tlcn.tlcn.train|\n",
      "+------------+--------------------+---------+------+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "s3a://mysql/brozes/train\n",
      "Save successfully\n"
     ]
    }
   ],
   "source": [
    "import json, os, re\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages io.delta:delta-spark_2.12:3.2.0,org.apache.hadoop:hadoop-common:3.3.4,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.apache.spark:spark-streaming-kafka-0-10_2.12:3.5.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 pyspark-shell'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import *\n",
    "\n",
    "from delta import *\n",
    "from delta.tables import *\n",
    "\n",
    "# Initialize Delta Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"write to minio\") \\\n",
    "    .master('spark://spark-master:7077') \\\n",
    "    .config('spark.executor.cores', '1') \\\n",
    "    .config('spark.executor.instances', '1') \\\n",
    "    .config('spark.driver.cores', '1') \\\n",
    "    .config('spark.cores.max', '3') \\\n",
    "    .config('spark.executor.memory', '512m') \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000/\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"before\", StringType(), True),\n",
    "    StructField(\"after\", StringType(), True)\n",
    "])\n",
    "\n",
    "def read_stream_from_kafka_topic(topic):\n",
    "    return (spark.readStream\n",
    "            .format(\"kafka\")\n",
    "            .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "            .option(\"subscribePattern\", topic)\n",
    "            .option(\"startingOffsets\", \"earliest\")\n",
    "            .option(\"failOnDataLoss\", \"false\")\n",
    "            .load()\n",
    "            # filter out empty values\n",
    "            .withColumn(\"value\", expr(\"string(value)\"))\n",
    "            .withColumn('value', \n",
    "                        from_json(col(\"value\"), schema))\n",
    "            .select(\n",
    "              # offset must be the first field, due to aggregation\n",
    "              expr(\"offset as kafka_offset\"),\n",
    "              expr(\"timestamp as kafka_ts\"),\n",
    "              expr(\"string(key) as kafka_key\"),\n",
    "              \"value.*\",\n",
    "              \"topic\"\n",
    "            )\n",
    "           )\n",
    "\n",
    "\n",
    "def get_schema_from_table(df):\n",
    "    df = df.select(\"after\")\n",
    "    df_read = spark.read.json(df.rdd.map(lambda x: x.after), multiLine=True)\n",
    "    schema = df_read.schema.json()\n",
    "    return StructType.fromJson(json.loads(schema))\n",
    "\n",
    "def write_to_delta(df, batch_id):\n",
    "    print(\"Data input\")\n",
    "    df.show()\n",
    "    delta_table_path = \"s3a://mysql/brozes/\"\n",
    "    topic_array = df.select(\"topic\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    unique_topics = list(set(topic_array))\n",
    "    for path in unique_topics:\n",
    "        name = path.split(\".\")[-1]\n",
    "        delta_paths = delta_table_path + name\n",
    "        print(delta_paths)\n",
    "\n",
    "        df_null_data = (df.filter(col(\"topic\") == path)\n",
    "            .select(\"kafka_key\", expr(\"struct(*) as d\"))\n",
    "            .groupBy(\"kafka_key\")\n",
    "            .agg(expr(\"max(d) d\")) \n",
    "            .select('d.kafka_key', \n",
    "                    'd.kafka_offset', \n",
    "                    'd.kafka_ts', \n",
    "                    'd.after',\n",
    "                    'd.before',\n",
    "                    'd.topic'\n",
    "            )\n",
    "            .filter(col(\"after\").isNull()))\n",
    "        \n",
    "        # print(\"Null data\")\n",
    "        # df_null_data.show()\n",
    "\n",
    "        df_notNull_data = (df.filter(col(\"topic\") == path)\n",
    "            .select(\"kafka_key\", expr(\"struct(*) as d\"))\n",
    "            .groupBy(\"kafka_key\")\n",
    "            .agg(expr(\"max(d) d\")) \n",
    "            .select('d.kafka_key', \n",
    "                    'd.kafka_offset', \n",
    "                    'd.kafka_ts', \n",
    "                    'd.after',\n",
    "                    'd.before',\n",
    "                    'd.topic'\n",
    "            )\n",
    "            .filter(col(\"after\").isNotNull()))\n",
    "        \n",
    "        # print(\"Not Null data\")\n",
    "        # df_notNull_data.show()\n",
    "        \n",
    "        if not df_notNull_data.rdd.isEmpty():\n",
    "            schema = get_schema_from_table(df_notNull_data.filter(col(\"topic\") == path))\n",
    "            df_notNull_data = (df_notNull_data\n",
    "                    .withColumn('after', from_json(col(\"after\"), schema))\n",
    "                    .select('kafka_key', \n",
    "                            'kafka_offset', \n",
    "                            'kafka_ts', \n",
    "                            'before',\n",
    "                            'after.*'\n",
    "                    ))\n",
    "            # Write to Delta Lake\n",
    "            if DeltaTable.isDeltaTable(spark, delta_paths):\n",
    "                \n",
    "                (DeltaTable\n",
    "                .forPath(spark, delta_paths)\n",
    "                .alias(\"now\")\n",
    "                .merge(df_notNull_data.alias(\"pre\"), \"pre.kafka_key = now.kafka_key\")\n",
    "                .whenMatchedUpdateAll()\n",
    "                .whenNotMatchedInsertAll()\n",
    "                .execute())\n",
    "            else:\n",
    "                (df_notNull_data\n",
    "                .write\n",
    "                .format(\"delta\")\n",
    "                .mode(\"append\")\n",
    "                .save(delta_paths))\n",
    "                \n",
    "            print(\"Save successfully\")\n",
    "        if not df_null_data.rdd.isEmpty():\n",
    "            \n",
    "            if DeltaTable.isDeltaTable(spark, delta_paths):\n",
    "                delta_table = DeltaTable.forPath(spark, delta_paths)\n",
    "\n",
    "                for row in df_null_data.collect():\n",
    "                    kafka_key = row[\"kafka_key\"]\n",
    "                    delta_table.delete(col(\"kafka_key\") == kafka_key)\n",
    "            print(\"Save successfully\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    topic = \"tlcn.tlcn.*\"\n",
    "    df = read_stream_from_kafka_topic(topic)\n",
    "\n",
    "    df.writeStream \\\n",
    "    .foreachBatch(write_to_delta) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start() \\\n",
    "    .awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbc7353-7113-4725-a0af-1fbddd3c3777",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
