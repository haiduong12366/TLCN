{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b346d6-1a7a-4f80-a2e3-a0a6abc7088b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install delta-spark==3.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de3d556-b47a-44f5-a1fa-1d6d631cf4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, re\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages io.delta:delta-spark_2.12:3.2.0,org.apache.hadoop:hadoop-common:3.3.4,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.apache.spark:spark-streaming-kafka-0-10_2.12:3.5.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 pyspark-shell'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import *\n",
    "\n",
    "from delta import *\n",
    "from delta.tables import *\n",
    "\n",
    "# Initialize Delta Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"write to minio\") \\\n",
    "    .master('spark://spark-master:7077') \\\n",
    "    .config('spark.executor.cores', '1') \\\n",
    "    .config('spark.executor.instances', '1') \\\n",
    "    .config('spark.driver.cores', '1') \\\n",
    "    .config('spark.cores.max', '3') \\\n",
    "    .config('spark.executor.memory', '512m') \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000/\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"before\", StringType(), True),\n",
    "    StructField(\"after\", StringType(), True)\n",
    "])\n",
    "\n",
    "def read_stream_from_kafka_topic(topic):\n",
    "    return (spark.readStream\n",
    "            .format(\"kafka\")\n",
    "            .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "            .option(\"subscribePattern\", topic)\n",
    "            .option(\"startingOffsets\", \"earliest\")\n",
    "            .option(\"failOnDataLoss\", \"false\")\n",
    "            .load()\n",
    "            # filter out empty values\n",
    "            .withColumn(\"value\", expr(\"string(value)\"))\n",
    "            .withColumn('value', \n",
    "                        from_json(col(\"value\"), schema))\n",
    "            .select(\n",
    "              # offset must be the first field, due to aggregation\n",
    "              expr(\"offset as kafka_offset\"),\n",
    "              expr(\"timestamp as kafka_ts\"),\n",
    "              expr(\"string(key) as kafka_key\"),\n",
    "              \"value.*\",\n",
    "              \"topic\"\n",
    "            )\n",
    "           )\n",
    "\n",
    "\n",
    "def get_schema_from_table(df):\n",
    "    df = df.select(\"after\")\n",
    "    df_read = spark.read.json(df.rdd.map(lambda x: x.after), multiLine=True)\n",
    "    schema = df_read.schema.json()\n",
    "    return StructType.fromJson(json.loads(schema))\n",
    "\n",
    "def write_to_delta(df, batch_id):\n",
    "    print(\"Data input\")\n",
    "    df.show()\n",
    "    delta_table_path = \"s3a://mysql/brozes/\"\n",
    "    topic_array = df.select(\"topic\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    unique_topics = list(set(topic_array))\n",
    "    for path in unique_topics:\n",
    "        name = path.split(\".\")[-1]\n",
    "        delta_paths = delta_table_path + name\n",
    "        print(delta_paths)\n",
    "\n",
    "        df_null_data = (df.filter(col(\"topic\") == path)\n",
    "            .select(\"kafka_key\", expr(\"struct(*) as d\"))\n",
    "            .groupBy(\"kafka_key\")\n",
    "            .agg(expr(\"max(d) d\")) \n",
    "            .select('d.kafka_key', \n",
    "                    'd.kafka_offset', \n",
    "                    'd.kafka_ts', \n",
    "                    'd.after',\n",
    "                    'd.before',\n",
    "                    'd.topic'\n",
    "            )\n",
    "            .filter(col(\"after\").isNull()))\n",
    "        \n",
    "        # print(\"Null data\")\n",
    "        # df_null_data.show()\n",
    "\n",
    "        df_notNull_data = (df.filter(col(\"topic\") == path)\n",
    "            .select(\"kafka_key\", expr(\"struct(*) as d\"))\n",
    "            .groupBy(\"kafka_key\")\n",
    "            .agg(expr(\"max(d) d\")) \n",
    "            .select('d.kafka_key', \n",
    "                    'd.kafka_offset', \n",
    "                    'd.kafka_ts', \n",
    "                    'd.after',\n",
    "                    'd.before',\n",
    "                    'd.topic'\n",
    "            )\n",
    "            .filter(col(\"after\").isNotNull()))\n",
    "        \n",
    "        # print(\"Not Null data\")\n",
    "        # df_notNull_data.show()\n",
    "        \n",
    "        if not df_notNull_data.rdd.isEmpty():\n",
    "            schema = get_schema_from_table(df_notNull_data.filter(col(\"topic\") == path))\n",
    "            df_notNull_data = (df_notNull_data\n",
    "                    .withColumn('after', from_json(col(\"after\"), schema))\n",
    "                    .select('kafka_key', \n",
    "                            'kafka_offset', \n",
    "                            'kafka_ts', \n",
    "                            'before',\n",
    "                            'after.*'\n",
    "                    ))\n",
    "            # Write to Delta Lake\n",
    "            if DeltaTable.isDeltaTable(spark, delta_paths):\n",
    "                \n",
    "                (DeltaTable\n",
    "                .forPath(spark, delta_paths)\n",
    "                .alias(\"now\")\n",
    "                .merge(df_notNull_data.alias(\"pre\"), \"pre.kafka_key = now.kafka_key\")\n",
    "                .whenMatchedUpdateAll()\n",
    "                .whenNotMatchedInsertAll()\n",
    "                .execute())\n",
    "            else:\n",
    "                (df_notNull_data\n",
    "                .write\n",
    "                .format(\"delta\")\n",
    "                .mode(\"append\")\n",
    "                .save(delta_paths))\n",
    "                \n",
    "            print(\"Save successfully\")\n",
    "        if not df_null_data.rdd.isEmpty():\n",
    "            \n",
    "            if DeltaTable.isDeltaTable(spark, delta_paths):\n",
    "                delta_table = DeltaTable.forPath(spark, delta_paths)\n",
    "\n",
    "                for row in df_null_data.collect():\n",
    "                    kafka_key = row[\"kafka_key\"]\n",
    "                    delta_table.delete(col(\"kafka_key\") == kafka_key)\n",
    "            print(\"Save successfully\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    topic = \"tlcn.tlcn.*\"\n",
    "    df = read_stream_from_kafka_topic(topic)\n",
    "\n",
    "    df.writeStream \\\n",
    "    .foreachBatch(write_to_delta) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start() \\\n",
    "    .awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
